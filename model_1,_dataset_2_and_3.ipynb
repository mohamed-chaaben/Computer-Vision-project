{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5E6XTWw_x_Fl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.models import vgg11\n",
        "from sklearn.metrics import precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmBNO0R0yBUC",
        "outputId": "f60984a5-2a29-4853-e038-3bec9086872a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "id": "XJ-CZwasligI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"/content/drive/MyDrive/Dataset_Project/Dataset 1.zip\"\n",
        "extracted_folder_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_1\"\n",
        "\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)"
      ],
      "metadata": {
        "id": "-F-GLa9klhUi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"/content/drive/MyDrive/Dataset_Project/Dataset 2.zip\"\n",
        "extracted_folder_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_2\"\n",
        "\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)"
      ],
      "metadata": {
        "id": "mdmSqFipmTB1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = \"/content/drive/MyDrive/Dataset_Project/Dataset 3.zip\"\n",
        "extracted_folder_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_3\"\n",
        "\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)"
      ],
      "metadata": {
        "id": "-gMJUv_4mhAC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VTg-jr262GrL"
      },
      "outputs": [],
      "source": [
        "data1_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_1/Dataset 1/Colorectal Cancer \"\n",
        "data2_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_2/Dataset 2/Prostate Cancer\"\n",
        "data3_path = \"/content/drive/MyDrive/Dataset_Project/Dataset_3/Dataset 3/Animal Faces\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "45SHly_52Esy"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "dataset1 = datasets.ImageFolder(data1_path, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset1))\n",
        "val_size = int(0.1 * len(dataset1))\n",
        "test_size = len(dataset1) - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(dataset1, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yEGck6LY2SV7"
      },
      "outputs": [],
      "source": [
        "model = vgg11(weights='DEFAULT')\n",
        "\n",
        "for param in model.features[:-5].parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes = 3\n",
        "model.classifier[6] = nn.Linear(4096, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBEw9ISI2X6z",
        "outputId": "30f60830-1be7-4276-87ac-6b0f4d30c8e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32)\n",
        "test_loader = DataLoader(test_set, batch_size=32)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 5\n",
        "no_improvement_count = 0\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print('start epoch ', epoch)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= patience:\n",
        "        print(f'Early stopping after {epoch + 1} epochs with no improvement.')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "print('Testing begin.')\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "all_labels = []\n",
        "all_predicted = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}%')\n",
        "print(f'Test Loss: {avg_test_loss}')\n",
        "print('Test Precision', precision)\n",
        "print('Test recall', recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzlOzT2LdG1I",
        "outputId": "359f3b99-3bf5-409b-9e54-f43579dff5fd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch  0\n",
            "Epoch 1/50 - Loss: 0.34232230550299086\n",
            "Validation Loss: 0.19745774410272898\n",
            "start epoch  1\n",
            "Epoch 2/50 - Loss: 0.16668879775330425\n",
            "Validation Loss: 0.12025732841146619\n",
            "start epoch  2\n",
            "Epoch 3/50 - Loss: 0.10458054318713646\n",
            "Validation Loss: 0.12941341504062476\n",
            "start epoch  3\n",
            "Epoch 4/50 - Loss: 0.0739613344706595\n",
            "Validation Loss: 0.0943769721412345\n",
            "start epoch  4\n",
            "Epoch 5/50 - Loss: 0.053862351470937334\n",
            "Validation Loss: 0.10565892705007603\n",
            "start epoch  5\n",
            "Epoch 6/50 - Loss: 0.05065047029250612\n",
            "Validation Loss: 0.1255147883570508\n",
            "start epoch  6\n",
            "Epoch 7/50 - Loss: 0.037614879292280724\n",
            "Validation Loss: 0.07405579569259364\n",
            "start epoch  7\n",
            "Epoch 8/50 - Loss: 0.02032962310302537\n",
            "Validation Loss: 0.0815734692457083\n",
            "start epoch  8\n",
            "Epoch 9/50 - Loss: 0.011097274348770347\n",
            "Validation Loss: 0.06807732745686448\n",
            "start epoch  9\n",
            "Epoch 10/50 - Loss: 0.005271138357131425\n",
            "Validation Loss: 0.07838925934323158\n",
            "start epoch  10\n",
            "Epoch 11/50 - Loss: 0.00662055506594091\n",
            "Validation Loss: 0.09270400946077548\n",
            "start epoch  11\n",
            "Epoch 12/50 - Loss: 0.005054714929477389\n",
            "Validation Loss: 0.0740622318362033\n",
            "start epoch  12\n",
            "Epoch 13/50 - Loss: 0.0070188654860733855\n",
            "Validation Loss: 0.1102092164862705\n",
            "start epoch  13\n",
            "Epoch 14/50 - Loss: 0.00568383675214136\n",
            "Validation Loss: 0.09336008914550276\n",
            "Early stopping after 14 epochs with no improvement.\n",
            "Training finished.\n",
            "Testing begin.\n",
            "Test Accuracy: 96.98492462311557%\n",
            "Test Loss: 0.15740543635451773\n",
            "Test Precision 0.9703982634438199\n",
            "Test recall 0.9698492462311558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2 Model 1 dataset 2:\n"
      ],
      "metadata": {
        "id": "mO83IppP49gu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7XQyK236_rp",
        "outputId": "c26e56a8-f085-48ff-a374-1bf00781dc48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (7): ReLU(inplace=True)\n",
              "  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (9): ReLU(inplace=True)\n",
              "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (12): ReLU(inplace=True)\n",
              "  (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (14): ReLU(inplace=True)\n",
              "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "model1 = nn.Sequential(*list(model.features.children())[:-5])\n",
        "model1.to(device)\n",
        "model1.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model12 = model1"
      ],
      "metadata": {
        "id": "1JPNxNM-2o9d"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = datasets.ImageFolder(data2_path, transform=transform)\n",
        "\n",
        "train_size2 = int(0.8 * len(dataset2))\n",
        "val_size2 = int(0.1 * len(dataset2))\n",
        "test_size2 = len(dataset2) - train_size2 - val_size2\n",
        "train_set2, val_set2, test_set2 = random_split(dataset2, [train_size2, val_size2, test_size2])"
      ],
      "metadata": {
        "id": "BIZ0a6Mhw2mA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set2, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set2, batch_size=32)\n",
        "test_loader = DataLoader(test_set2, batch_size=32)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model12.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 5\n",
        "no_improvement_count = 0\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print('start epoch ', epoch)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= patience:\n",
        "        print(f'Early stopping after {epoch + 1} epochs with no improvement.')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "print('Testing begin.')\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "all_labels = []\n",
        "all_predicted = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}%')\n",
        "print(f'Test Loss: {avg_test_loss}')\n",
        "print('Test Precision', precision)\n",
        "print('Test recall', recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHkfFckJwHxA",
        "outputId": "2dbfa41a-e218-4b9a-ffff-676352202f31"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch  0\n",
            "Epoch 1/50 - Loss: 2.6360572735468546\n",
            "Validation Loss: 2.5516080668098047\n",
            "start epoch  1\n",
            "Epoch 2/50 - Loss: 2.6124315985043842\n",
            "Validation Loss: 2.5516080668098047\n",
            "start epoch  2\n",
            "Epoch 3/50 - Loss: 2.639640770753225\n",
            "Validation Loss: 2.5516080668098047\n",
            "start epoch  3\n",
            "Epoch 4/50 - Loss: 2.626651151974996\n",
            "Validation Loss: 2.5516080668098047\n",
            "start epoch  4\n",
            "Epoch 5/50 - Loss: 2.5963445965449017\n",
            "Validation Loss: 2.5516080668098047\n",
            "start epoch  5\n",
            "Epoch 6/50 - Loss: 2.59914825518926\n",
            "Validation Loss: 2.5516080668098047\n",
            "Early stopping after 6 epochs with no improvement.\n",
            "Training finished.\n",
            "Testing begin.\n",
            "Test Accuracy: 41.333333333333336%\n",
            "Test Loss: 2.5569603631370947\n",
            "Test Precision 0.48214632816849134\n",
            "Test recall 0.41333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 Dataset 3"
      ],
      "metadata": {
        "id": "HeCsFjs85E12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model13 = model1"
      ],
      "metadata": {
        "id": "XL1AGAV35_9r"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset3 = datasets.ImageFolder(data3_path, transform=transform)\n",
        "\n",
        "train_size3 = int(0.8 * len(dataset3))\n",
        "val_size3 = int(0.1 * len(dataset3))\n",
        "test_size3 = len(dataset3) - train_size3 - val_size3\n",
        "train_set3, val_set3, test_set3 = random_split(dataset3, [train_size3, val_size3, test_size3])"
      ],
      "metadata": {
        "id": "YYo8gm7AxI1r"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set3, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set3, batch_size=32)\n",
        "test_loader = DataLoader(test_set3, batch_size=32)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model13.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 5\n",
        "no_improvement_count = 0\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print('start epoch ', epoch)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= patience:\n",
        "        print(f'Early stopping after {epoch + 1} epochs with no improvement.')\n",
        "        break\n",
        "\n",
        "print('Training finished.')\n",
        "print('Testing begin.')\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "all_labels = []\n",
        "all_predicted = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
        "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
        "\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}%')\n",
        "print(f'Test Loss: {avg_test_loss}')\n",
        "print('Test Precision', precision)\n",
        "print('Test recall', recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzo3furmxIms",
        "outputId": "eda00ef1-07f9-4145-b52b-5c2461e77b12"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch  0\n",
            "Epoch 1/50 - Loss: 3.3903044255574546\n",
            "Validation Loss: 2.967450236019335\n",
            "start epoch  1\n",
            "Epoch 2/50 - Loss: 3.43419419447581\n",
            "Validation Loss: 2.967450236019335\n",
            "start epoch  2\n",
            "Epoch 3/50 - Loss: 3.429710636138916\n",
            "Validation Loss: 2.967450236019335\n",
            "start epoch  3\n",
            "Epoch 4/50 - Loss: 3.4225958132743837\n",
            "Validation Loss: 2.967450236019335\n",
            "start epoch  4\n",
            "Epoch 5/50 - Loss: 3.4084727581342062\n",
            "Validation Loss: 2.967450236019335\n",
            "start epoch  5\n",
            "Epoch 6/50 - Loss: 3.40571515083313\n",
            "Validation Loss: 2.967450236019335\n",
            "Early stopping after 6 epochs with no improvement.\n",
            "Training finished.\n",
            "Testing begin.\n",
            "Test Accuracy: 37.5%\n",
            "Test Loss: 3.041347051921644\n",
            "Test Precision 0.4093263596781973\n",
            "Test recall 0.375\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}